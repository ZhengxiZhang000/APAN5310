{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902c0b52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy==1.3.24\n",
      "  Downloading SQLAlchemy-1.3.24.tar.gz (6.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: sqlalchemy\n",
      "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sqlalchemy: filename=SQLAlchemy-1.3.24-cp311-cp311-macosx_10_9_x86_64.whl size=1213833 sha256=22ef0331f58245c72c2863be66762e7388482821009cdf4eedcade3e172eea95\n",
      "  Stored in directory: /Users/zhengxizhang/Library/Caches/pip/wheels/27/e3/c5/7ba1f3cf983a93b35c849e97fd51a82aa7adf440641a2f690b\n",
      "Successfully built sqlalchemy\n",
      "Installing collected packages: sqlalchemy\n",
      "  Attempting uninstall: sqlalchemy\n",
      "    Found existing installation: SQLAlchemy 2.0.31\n",
      "    Uninstalling SQLAlchemy-2.0.31:\n",
      "      Successfully uninstalled SQLAlchemy-2.0.31\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "uszipcode 0.2.5 requires SQLAlchemy>=1.4.0, but you have sqlalchemy 1.3.24 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed sqlalchemy-1.3.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy==1.3.24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3c83fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'officeid': 200, 'name': 200, 'address': 200, 'city': 200, 'state': 200, 'zipcode': 200, 'numemployees': 200}\n",
      "CSV file generated\n"
     ]
    }
   ],
   "source": [
    "#offices\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# List of states and cities to be generated\n",
    "states = ['NY', 'NJ', 'CT']\n",
    "cities = {\n",
    "    'NY': ['New York', 'Buffalo', 'Rochester', 'Yonkers', 'Syracuse', 'Albany', 'White Plains', 'Ithaca'],\n",
    "    'NJ': ['Newark', 'Jersey City', 'Paterson', 'Elizabeth', 'Edison', 'Trenton', 'Camden', 'Clifton'],\n",
    "    'CT': ['Bridgeport', 'New Haven', 'Stamford', 'Hartford', 'Waterbury', 'Norwalk', 'Danbury', 'New Britain']\n",
    "}\n",
    "\n",
    "# Predefined list of postal codes for each state (example data, ensure it matches real data)\n",
    "zipcodes = {\n",
    "    'NY': ['10001', '14201', '14604', '10701', '13201', '12207', '10601', '14850'],\n",
    "    'NJ': ['07102', '07302', '07501', '07201', '08817', '08608', '08102', '07013'],\n",
    "    'CT': ['06604', '06510', '06901', '06103', '06702', '06851', '06810', '06053']\n",
    "}\n",
    "\n",
    "# Define a function to generate random data and save it to a CSV file\n",
    "def generate_office_data_csv(filename, num_rows):\n",
    "    data = {\n",
    "        'officeid': [],  # Add officeid column\n",
    "        'name': [],\n",
    "        'address': [],\n",
    "        'city': [],\n",
    "        'state': [],\n",
    "        'zipcode': [],\n",
    "        'numemployees': []\n",
    "    }\n",
    "\n",
    "    # Generate a list of unique four-digit office IDs\n",
    "    office_ids = random.sample(range(1000, 10000), num_rows)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly select a state\n",
    "        state = random.choice(states)\n",
    "\n",
    "        # Randomly select a city\n",
    "        city = random.choice(cities[state])\n",
    "\n",
    "        # Randomly select a postal code\n",
    "        zipcode = random.choice(zipcodes[state])\n",
    "\n",
    "        # Populate data\n",
    "        data['officeid'].append(office_ids[i])  # Assign a unique four-digit office ID\n",
    "        data['name'].append(fake.company())\n",
    "        data['address'].append(fake.street_address())\n",
    "        data['city'].append(city)\n",
    "        data['state'].append(state)\n",
    "        data['zipcode'].append(zipcode)\n",
    "        data['numemployees'].append(np.random.randint(1, 100))\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 200 rows of data\n",
    "generate_office_data_csv('office_data.csv', 200)\n",
    "\n",
    "print(\"CSV file generated\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1d280b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expenses.csv file generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Fake expenses\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Read data from the Office table\n",
    "office_df = pd.read_csv('office_data.csv')\n",
    "\n",
    "# Get the list of existing OfficeIDs\n",
    "office_ids = office_df['officeid'].tolist()\n",
    "\n",
    "# Generate data for the Expenses table\n",
    "def generate_expenses_data(num_records, office_ids):\n",
    "    expenses = []\n",
    "    for _ in range(num_records):\n",
    "        expenses.append({\n",
    "            \"expenseid\": _ + 1,  # Assume an auto-incrementing ID starting from 1\n",
    "            \"officeid\": random.choice(office_ids),  # Randomly select an OfficeID\n",
    "            \"expensedate\": fake.date_this_decade(),\n",
    "            \"expensetype\": random.choice([\"Utilities\", \"Rent\", \"Salaries\", \"Supplies\", \"Maintenance\"]),\n",
    "            \"amount\": round(random.uniform(100, 10000), 2),\n",
    "            \"description\": fake.text(max_nb_chars=200)\n",
    "        })\n",
    "    return pd.DataFrame(expenses)\n",
    "\n",
    "# Define the number of records to generate\n",
    "num_expenses = 800\n",
    "\n",
    "# Generate data and save as a CSV file\n",
    "expenses_df = generate_expenses_data(num_expenses, office_ids)\n",
    "expenses_df.to_csv('expenses_data.csv', index=False)\n",
    "\n",
    "print(\"Expenses.csv file generated successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "177fc2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'userid': 1048, 'firstname': 1048, 'lastname': 1048, 'dateofbirth': 1048, 'phonenumber': 1048, 'email': 1048, 'address': 1048, 'city': 1048, 'state': 1048, 'zipcode': 1048, 'type': 1048}\n",
      "User CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#users\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "import pgeocode\n",
    "\n",
    "# Initialize Faker and pgeocode\n",
    "fake = Faker('en_US')\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "# Define a function to generate random user data and save it to a CSV file\n",
    "def generate_user_data_csv(filename, num_rows):\n",
    "    data = {\n",
    "        'userid': [],          # User ID\n",
    "        'firstname': [],       # First Name\n",
    "        'lastname': [],        # Last Name\n",
    "        'dateofbirth': [],     # Date of Birth\n",
    "        'phonenumber': [],     # Phone Number\n",
    "        'email': [],           # Email\n",
    "        'address': [],         # Address\n",
    "        'city': [],            # City\n",
    "        'state': [],           # State\n",
    "        'zipcode': [],         # ZIP Code\n",
    "        'type': []             # Type\n",
    "    }\n",
    "\n",
    "    user_types = ['Admin', 'Customer', 'Employee', 'Guest']  # Define user types\n",
    "\n",
    "    # Generate a list of unique 5-digit user IDs\n",
    "    user_ids = random.sample(range(10000, 100000), num_rows)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Generate a random ZIP code\n",
    "        zipcode = fake.zipcode()\n",
    "        \n",
    "        # Use pgeocode to get the city and state for the ZIP code\n",
    "        location = nomi.query_postal_code(zipcode)\n",
    "        if pd.notnull(location.place_name):\n",
    "            city = location.place_name.split(',')[0]\n",
    "            state_code = location.state_code\n",
    "\n",
    "            # Populate data\n",
    "            data['userid'].append(user_ids[i])  # Assign a unique 5-digit user ID\n",
    "            data['firstname'].append(fake.first_name())\n",
    "            data['lastname'].append(fake.last_name())\n",
    "            data['dateofbirth'].append(fake.date_of_birth(minimum_age=18, maximum_age=70))\n",
    "            \n",
    "            # Generate a 10-digit US phone number\n",
    "            phone_number = fake.msisdn()[-10:]  # Extract the last 10 digits\n",
    "            phone_number = f\"({phone_number[:3]}) {phone_number[3:6]}-{phone_number[6:]}\"\n",
    "            data['phonenumber'].append(phone_number)\n",
    "\n",
    "            # Generate a realistic email\n",
    "            email = f\"{data['firstname'][-1].lower()}.{data['lastname'][-1].lower()}{np.random.randint(1, 100)}@example.com\"\n",
    "            data['email'].append(email)\n",
    "\n",
    "            data['address'].append(fake.street_address())\n",
    "            data['city'].append(city)\n",
    "            data['state'].append(state_code)  # Use state abbreviation (state_code)\n",
    "            data['zipcode'].append(zipcode)\n",
    "            data['type'].append(random.choice(user_types))\n",
    "\n",
    "    # Create a DataFrame to hold the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 1000 rows of user data\n",
    "generate_user_data_csv('users_data.csv', 2500)\n",
    "\n",
    "print(\"User CSV file generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ebf9235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'agentid': 1000, 'officeid': 1000, 'firstname': 1000, 'lastname': 1000, 'phonenumber': 1000, 'email': 1000, 'licensenumber': 1000, 'yearsofexperience': 1000}\n",
      "Agent CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#fake agent\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "import pgeocode\n",
    "\n",
    "# Initialize Faker and pgeocode\n",
    "fake = Faker('en_US')\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "# Read OfficeID from the already generated office_data.csv\n",
    "office_df = pd.read_csv('office_data.csv')\n",
    "office_ids = office_df['officeid'].tolist()\n",
    "\n",
    "# Define a function to generate random agent data and save it to a CSV file\n",
    "def generate_agent_data_csv(filename, num_rows, office_ids):\n",
    "    data = {\n",
    "        'agentid': [],           # Agent ID\n",
    "        'officeid': [],          # Office ID\n",
    "        'firstname': [],         # First Name\n",
    "        'lastname': [],          # Last Name\n",
    "        'phonenumber': [],       # Phone Number\n",
    "        'email': [],             # Email\n",
    "        'licensenumber': [],     # License Number\n",
    "        'yearsofexperience': []  # Years of Experience\n",
    "        #'employmentstatus': []   # Employment Status\n",
    "    }\n",
    "\n",
    "    #employment_statuses = ['Full-time', 'Part-time', 'Contract']  # Define employment statuses\n",
    "    agent_ids = random.sample(range(10000, 100000), num_rows)  # Generate unique five-digit agent IDs\n",
    "\n",
    "    # Common email domains\n",
    "    common_domains = [\n",
    "        'gmail.com', 'yahoo.com', 'outlook.com', 'hotmail.com',\n",
    "        'aol.com', 'comcast.net', 'icloud.com', 'mail.com'\n",
    "    ]\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly select a valid OfficeID\n",
    "        officeid = random.choice(office_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['agentid'].append(agent_ids[i])\n",
    "        data['officeid'].append(officeid)\n",
    "        data['firstname'].append(fake.first_name())\n",
    "        data['lastname'].append(fake.last_name())\n",
    "\n",
    "        # Generate a 10-digit US phone number\n",
    "        phone_number = fake.msisdn()[-10:]  # Get the last 10 digits from the generated msisdn\n",
    "        phone_number = f\"({phone_number[:3]}) {phone_number[3:6]}-{phone_number[6:]}\"\n",
    "        data['phonenumber'].append(phone_number)\n",
    "\n",
    "        # Generate realistic email\n",
    "        domain = random.choice(common_domains)\n",
    "        first_name = data['firstname'][-1].lower()\n",
    "        last_name = data['lastname'][-1].lower()\n",
    "        username_formats = [\n",
    "            f\"{first_name}.{last_name}\",\n",
    "            f\"{first_name}{last_name}\",\n",
    "            f\"{first_name}_{last_name}\",\n",
    "            f\"{first_name[0]}{last_name}\",\n",
    "            f\"{last_name}{first_name[0]}\"\n",
    "        ]\n",
    "        username = random.choice(username_formats)\n",
    "        email = f\"{username}@{domain}\"\n",
    "        data['email'].append(email)\n",
    "\n",
    "        data['licensenumber'].append(fake.bothify(text='??##-########'))\n",
    "        data['yearsofexperience'].append(round(random.uniform(1, 30), 2))  # Generate 1 to 30 years of experience, rounded to two decimal places\n",
    "        #data['employmentstatus'].append(random.choice(employment_statuses))\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 1000 rows of agent data\n",
    "generate_agent_data_csv('agent_data.csv', 1000, office_ids)\n",
    "\n",
    "print(\"Agent CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "903cd5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'agentid': 1000, 'userid': 1000}\n",
      "Services CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#services\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Read existing Agent and User IDs\n",
    "agents_df = pd.read_csv('agent_data.csv')\n",
    "users_df = pd.read_csv('users_data.csv')\n",
    "\n",
    "agent_ids = agents_df['agentid'].tolist()\n",
    "user_ids = users_df['userid'].tolist()\n",
    "\n",
    "# Define a function to generate service data and save it to a CSV file\n",
    "def generate_services_data_csv(filename, num_rows, agent_ids, user_ids):\n",
    "    data = {\n",
    "        'agentid': [],\n",
    "        'userid': []\n",
    "    }\n",
    "\n",
    "    # Use a set to track unique (AgentID, UserID) pairs to ensure no duplicates\n",
    "    existing_pairs = set()\n",
    "\n",
    "    while len(data['agentid']) < num_rows:\n",
    "        # Randomly select an AgentID and UserID\n",
    "        agent_id = random.choice(agent_ids)\n",
    "        user_id = random.choice(user_ids)\n",
    "\n",
    "        # Create a pair\n",
    "        pair = (agent_id, user_id)\n",
    "\n",
    "        # Only add unique pairs\n",
    "        if pair not in existing_pairs:\n",
    "            existing_pairs.add(pair)\n",
    "            data['agentid'].append(agent_id)\n",
    "            data['userid'].append(user_id)\n",
    "\n",
    "    # Create a DataFrame to hold the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 500 rows of service data\n",
    "generate_services_data_csv('services_data.csv', 1000, agent_ids, user_ids)\n",
    "\n",
    "print(\"Services CSV file generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28039382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'sellerid': 800, 'userid': 800, 'askingprice': 800, 'sellingyears': 800, 'numlistings': 800}\n",
      "Sellers CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#fake seller\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# Read UserID from the already generated user_data.csv\n",
    "users_df = pd.read_csv('users_data.csv')\n",
    "user_ids = users_df['userid'].tolist()\n",
    "\n",
    "# Define a function to generate random seller data and save it to a CSV file\n",
    "def generate_seller_data_csv(filename, num_rows, user_ids):\n",
    "    data = {\n",
    "        'sellerid': [],       # Seller ID\n",
    "        'userid': [],         # User ID\n",
    "        'askingprice': [],    # Asking Price\n",
    "        'sellingyears': [],   # Years of Selling\n",
    "        'numlistings': []     # Number of Listings\n",
    "    }\n",
    "\n",
    "    # Generate a list of unique six-digit seller IDs\n",
    "    seller_ids = random.sample(range(100000, 1000000), num_rows)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly select a valid UserID\n",
    "        userid = random.choice(user_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['sellerid'].append(seller_ids[i])\n",
    "        data['userid'].append(userid)\n",
    "        data['askingprice'].append(round(random.uniform(50000, 1000000), 2))  # Generate a random asking price between 50,000 and 1,000,000\n",
    "        data['sellingyears'].append(round(random.uniform(1, 20), 2))  # Generate years of selling between 1 and 20, rounded to two decimal places\n",
    "        data['numlistings'].append(random.randint(1, 50))  # Generate a random number of listings between 1 and 50\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 800 rows of seller data\n",
    "generate_seller_data_csv('sellers_data.csv', 800, user_ids)\n",
    "\n",
    "print(\"Sellers CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88d1b3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'buyerid': 800, 'userid': 800, 'preapprovalstatus': 800, 'budget': 800, 'preferredpropertytype': 800, 'desiredlocation': 800, 'moveindate': 800, 'petstatus': 800}\n",
      "Buyers CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#fake buyers\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# Read User IDs from existing users_data.csv\n",
    "users_df = pd.read_csv('users_data.csv')\n",
    "user_ids = users_df['userid'].tolist()\n",
    "\n",
    "# Define a function to generate random buyer data and save it to a CSV file\n",
    "def generate_buyer_data_csv(filename, num_rows, user_ids):\n",
    "    data = {\n",
    "        'buyerid': [],             # Buyer ID\n",
    "        'userid': [],              # User ID\n",
    "        'preapprovalstatus': [],   # Pre-approval Status (True or False)\n",
    "        'budget': [],              # Budget\n",
    "        'preferredpropertytype': [], # Preferred Property Type\n",
    "        'desiredlocation': [],     # Desired Location\n",
    "        'moveindate': [],          # Move-in Date\n",
    "        'petstatus': []            # Pet Status (True or False)\n",
    "    }\n",
    "\n",
    "    property_types = ['Condo', 'Single Family', 'Townhouse', 'Multi-Family']\n",
    "    locations = ['Downtown', 'Suburbs', 'Rural Area']\n",
    "\n",
    "    # Generate a set of unique Buyer IDs\n",
    "    existing_buyer_ids = set()\n",
    "\n",
    "    def generate_unique_buyer_id():\n",
    "        while True:\n",
    "            new_id = random.randint(100000, 999999)  # Generate a six-digit ID\n",
    "            if new_id not in existing_buyer_ids:\n",
    "                existing_buyer_ids.add(new_id)\n",
    "                return new_id\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly choose a valid User ID\n",
    "        user_id = random.choice(user_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['buyerid'].append(generate_unique_buyer_id())\n",
    "        data['userid'].append(user_id)\n",
    "        data['preapprovalstatus'].append(random.choice([True, False]))  # Assign True or False\n",
    "        data['budget'].append(round(random.uniform(50000, 2000000), 2))  # Generate a budget between 50k and 2M\n",
    "        data['preferredpropertytype'].append(random.choice(property_types))\n",
    "        data['desiredlocation'].append(random.choice(locations))\n",
    "        data['moveindate'].append(fake.date_between(start_date='today', end_date='+1y'))\n",
    "        data['petstatus'].append(random.choice([True, False]))  # Assign True or False\n",
    "\n",
    "    # Create a DataFrame to hold the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a buyer CSV file with 300 rows of data\n",
    "generate_buyer_data_csv('buyers_data.csv', 800, user_ids)\n",
    "\n",
    "print(\"Buyers CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5c86f1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'propertyid': 886, 'city': 886, 'state': 886, 'zipcode': 886, 'address': 886, 'bedrooms': 886, 'bathrooms': 886, 'squarefeet': 886, 'pricing': 886, 'yearbuilt': 886, 'description': 886, 'listingdate': 886, 'status': 886, 'type': 886}\n",
      "Properties CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#properties\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "import pgeocode\n",
    "\n",
    "# Initialize Faker and pgeocode\n",
    "fake = Faker('en_US')\n",
    "nomi = pgeocode.Nominatim('us')\n",
    "\n",
    "# Define a function to generate random property data and save it to a CSV file\n",
    "def generate_property_data_csv(filename, num_rows):\n",
    "    data = {\n",
    "        'propertyid': [],   # Property ID\n",
    "        'city': [],         # City\n",
    "        'state': [],        # State\n",
    "        'zipcode': [],      # ZIP Code\n",
    "        'address': [],      # Address\n",
    "        'bedrooms': [],     # Number of Bedrooms\n",
    "        'bathrooms': [],    # Number of Bathrooms\n",
    "        'squarefeet': [],   # Square Feet\n",
    "        'pricing': [],      # Pricing\n",
    "        'yearbuilt': [],    # Year Built\n",
    "        'description': [],  # Description\n",
    "        'listingdate': [],  # Listing Date\n",
    "        'status': [],       # Status\n",
    "        'type': []          # Type\n",
    "    }\n",
    "\n",
    "    statuses = ['For Sale', 'Sold', 'Pending', 'Under Contract', 'Off Market']  # Property statuses\n",
    "    types = ['Residential', 'Commercial', 'Land', 'Industrial', 'Mixed-Use']  # Property types\n",
    "    descriptions = [\n",
    "        'A beautiful home with stunning views.',\n",
    "        'Located in a quiet neighborhood, this property offers great potential.',\n",
    "        'Recently renovated with modern finishes.',\n",
    "        'Spacious property with a large backyard.',\n",
    "        'Close to schools and shopping centers.',\n",
    "        'Perfect for first-time buyers or investors.'\n",
    "    ]\n",
    "\n",
    "    # Generate a list of unique 8-digit property IDs\n",
    "    property_ids = random.sample(range(10000000, 100000000), num_rows)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Generate a random ZIP code\n",
    "        zipcode = fake.zipcode()\n",
    "\n",
    "        # Use pgeocode to get the city and state for the ZIP code\n",
    "        location = nomi.query_postal_code(zipcode)\n",
    "        if pd.notnull(location.place_name):\n",
    "            city = location.place_name.split(',')[0]  # Use the first city name if multiple exist\n",
    "            state = location.state_name\n",
    "\n",
    "            # Populate data\n",
    "            data['propertyid'].append(property_ids[i])\n",
    "            data['city'].append(city)\n",
    "            data['state'].append(state)\n",
    "            data['zipcode'].append(zipcode)\n",
    "            data['address'].append(fake.street_address())\n",
    "            data['bedrooms'].append(random.randint(1, 5))  # Generate between 1 to 5 bedrooms\n",
    "            data['bathrooms'].append(random.randint(1, 3))  # Generate between 1 to 3 bathrooms\n",
    "            data['squarefeet'].append(random.randint(500, 5000))  # Generate between 500 to 5000 square feet\n",
    "            data['pricing'].append(round(random.uniform(50000, 5000000), 2))  # Generate pricing between 50k to 5M\n",
    "            data['yearbuilt'].append(random.randint(1900, 2023))  # Generate a year built between 1900 and 2023\n",
    "            data['description'].append(random.choice(descriptions))\n",
    "            data['listingdate'].append(fake.date_between(start_date='-1y', end_date='today'))  # Past year's date\n",
    "            data['status'].append(random.choice(statuses))\n",
    "            data['type'].append(random.choice(types))\n",
    "\n",
    "    # Create a DataFrame to hold the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 2000 rows of property data\n",
    "generate_property_data_csv('properties_data.csv', 2200)\n",
    "\n",
    "print(\"Properties CSV file generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a44a9844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   propertyid         city     state  zipcode                         address  \\\n",
      "0    87623441      Challis     Idaho    83226  8109 Renee Crossroad Suite 965   \n",
      "1    22759467  Saint Louis  Missouri    63127                 0692 Chris Well   \n",
      "2    98383107      Lubbock     Texas    79430            28985 Spencer Stream   \n",
      "3    54113232      Bedford     Texas    76022       6500 Spears Way Suite 199   \n",
      "4    77072907       Toledo      Ohio    43666          81089 Rickey Junctions   \n",
      "\n",
      "   bedrooms  bathrooms  squarefeet     pricing  yearbuilt  \\\n",
      "0         4          3        1823  1617931.37       2019   \n",
      "1         5          2        4514  3315748.79       1964   \n",
      "2         4          3        2857   613700.54       1926   \n",
      "3         2          3         790   601140.74       1938   \n",
      "4         3          3        3187  1878261.83       1985   \n",
      "\n",
      "                                   description listingdate      status  \\\n",
      "0  Perfect for first-time buyers or investors.  2024-04-26  Off Market   \n",
      "1  Perfect for first-time buyers or investors.  2024-02-03     Pending   \n",
      "2        A beautiful home with stunning views.  2024-04-06  Off Market   \n",
      "3     Spacious property with a large backyard.  2024-05-25  Off Market   \n",
      "4  Perfect for first-time buyers or investors.  2023-12-26  Off Market   \n",
      "\n",
      "            type  \n",
      "0     townhouses  \n",
      "1  single houses  \n",
      "2     apartments  \n",
      "3   condominiums  \n",
      "4         villas  \n"
     ]
    }
   ],
   "source": [
    "#修改一下properties的type类型\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data from a CSV file\n",
    "df = pd.read_csv('properties_data.csv')\n",
    "\n",
    "# Define the new property types\n",
    "property_types = ['single houses', 'townhouses', 'apartments', 'villas', 'condominiums']\n",
    "\n",
    "# Randomly assign a new type to each row in the 'type' column\n",
    "df['type'] = np.random.choice(property_types, size=len(df))\n",
    "\n",
    "# Optionally save the modified DataFrame back to a CSV file\n",
    "df.to_csv('modified_data.csv', index=False)\n",
    "\n",
    "# Print the modified DataFrame to verify changes\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c617e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'imageid': 600, 'imageurl': 600, 'description': 600}\n",
      "PropertyImages CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#propertyimages\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Define a function to generate random property image data and save it to a CSV file\n",
    "def generate_property_images_csv(filename, num_rows):\n",
    "    data = {\n",
    "        'imageid': [],       # Image ID\n",
    "        'imageurl': [],      # Image URL\n",
    "        'description': []    # Description\n",
    "    }\n",
    "\n",
    "    descriptions = [\n",
    "        'Front view of the property',\n",
    "        'Backyard with a garden',\n",
    "        'Living room with modern furniture',\n",
    "        'Spacious kitchen area',\n",
    "        'Master bedroom with ensuite bathroom',\n",
    "        'Swimming pool and patio',\n",
    "        'Garage and driveway view',\n",
    "        'Dining room with a large table',\n",
    "        'Panoramic view from the balcony',\n",
    "        'Elegant home office setup'\n",
    "    ]\n",
    "\n",
    "    # Generate a list of unique Image IDs\n",
    "    image_ids = random.sample(range(10000000, 100000000), num_rows)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Populate data\n",
    "        data['imageid'].append(image_ids[i])  # Unique Image ID\n",
    "        data['imageurl'].append(fake.image_url(width=640, height=480))  # Random image URL\n",
    "        data['description'].append(random.choice(descriptions))  # Random description\n",
    "\n",
    "    # Create a DataFrame to hold the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 500 rows of property image data\n",
    "generate_property_images_csv('property_images_data.csv', 600)\n",
    "\n",
    "print(\"PropertyImages CSV file generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c327f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'listingid': 800, 'imageid': 800, 'agentid': 800, 'sellerid': 800, 'propertyid': 800}\n",
      "Listings CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#listing\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# Read IDs from already generated CSV files\n",
    "agents_df = pd.read_csv('agent_data.csv')\n",
    "sellers_df = pd.read_csv('sellers_data.csv')\n",
    "properties_df = pd.read_csv('properties_data.csv')\n",
    "images_df = pd.read_csv('property_images_data.csv')\n",
    "\n",
    "agent_ids = agents_df['agentid'].tolist()\n",
    "seller_ids = sellers_df['sellerid'].tolist()\n",
    "property_ids = properties_df['propertyid'].tolist()\n",
    "image_ids = images_df['imageid'].tolist()\n",
    "\n",
    "# Define a function to generate random listing data and save it to a CSV file\n",
    "def generate_listing_data_csv(filename, num_rows, agent_ids, seller_ids, property_ids, image_ids):\n",
    "    data = {\n",
    "        'listingid': [],  # Listing ID\n",
    "        'imageid': [],    # Image ID\n",
    "        'agentid': [],    # Agent ID\n",
    "        'sellerid': [],   # Seller ID\n",
    "        'propertyid': []  # Property ID\n",
    "    }\n",
    "\n",
    "    # Use a set to ensure unique ListingIDs\n",
    "    existing_listing_ids = set()\n",
    "\n",
    "    def generate_unique_listing_id():\n",
    "        while True:\n",
    "            new_id = random.randint(100000000, 999999999)  # Generate a nine-digit ID\n",
    "            if new_id not in existing_listing_ids:\n",
    "                existing_listing_ids.add(new_id)\n",
    "                return new_id\n",
    "\n",
    "    for _ in range(num_rows):\n",
    "        # Randomly select a valid AgentID, SellerID, PropertyID, and ImageID\n",
    "        agent_id = random.choice(agent_ids)\n",
    "        seller_id = random.choice(seller_ids)\n",
    "        property_id = random.choice(property_ids)\n",
    "        image_id = random.choice(image_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['listingid'].append(generate_unique_listing_id())\n",
    "        data['imageid'].append(image_id)\n",
    "        data['agentid'].append(agent_id)\n",
    "        data['sellerid'].append(seller_id)\n",
    "        data['propertyid'].append(property_id)\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 800 rows of listing data\n",
    "generate_listing_data_csv('listings_data.csv', 800, agent_ids, seller_ids, property_ids, image_ids)\n",
    "\n",
    "print(\"Listings CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df4b3ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'openhouseid': 500, 'listingid': 500, 'date': 500, 'starttime': 500, 'endtime': 500, 'attendees': 500}\n",
      "OpenHouses CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#fake openhouse\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# Read ListingID from the already generated listings_data.csv\n",
    "listings_df = pd.read_csv('listings_data.csv')\n",
    "listing_ids = listings_df['listingid'].tolist()\n",
    "\n",
    "# Define a function to generate random open house data and save it to a CSV file\n",
    "def generate_openhouse_data_csv(filename, num_rows, listing_ids):\n",
    "    data = {\n",
    "        'openhouseid': [],  # Open House ID\n",
    "        'listingid': [],    # Listing ID\n",
    "        'date': [],         # Date\n",
    "        'starttime': [],    # Start Time\n",
    "        'endtime': [],      # End Time\n",
    "        'attendees': []     # Attendees\n",
    "    }\n",
    "\n",
    "    # Generate a list of unique six-digit open house IDs\n",
    "    openhouse_ids = random.sample(range(100000, 1000000), num_rows)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly select a valid ListingID\n",
    "        listing_id = random.choice(listing_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['openhouseid'].append(openhouse_ids[i])\n",
    "        data['listingid'].append(listing_id)\n",
    "        data['date'].append(fake.date_between(start_date='-1y', end_date='today'))\n",
    "        start_time = fake.time_object(end_datetime=None)\n",
    "        end_time = fake.time_object(end_datetime=None)\n",
    "        \n",
    "        # Ensure the end time is later than the start time\n",
    "        while end_time <= start_time:\n",
    "            end_time = fake.time_object(end_datetime=None)\n",
    "        \n",
    "        data['starttime'].append(start_time.strftime('%H:%M:%S'))\n",
    "        data['endtime'].append(end_time.strftime('%H:%M:%S'))\n",
    "        data['attendees'].append(', '.join(fake.name() for _ in range(random.randint(5, 20))))\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 500 rows of open house data\n",
    "generate_openhouse_data_csv('openhouses_data.csv', 500, listing_ids)\n",
    "\n",
    "print(\"OpenHouses CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a236b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'appointmentid': 600, 'buyerid': 600, 'listingid': 600, 'date': 600, 'starttime': 600, 'endtime': 600}\n",
      "Appointment CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#fake appointment\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# Read IDs from already generated buyers_data.csv and listings_data.csv\n",
    "buyers_df = pd.read_csv('buyers_data.csv')\n",
    "listings_df = pd.read_csv('listings_data.csv')\n",
    "\n",
    "buyer_ids = buyers_df['buyerid'].tolist()\n",
    "listing_ids = listings_df['listingid'].tolist()\n",
    "\n",
    "# Define a function to generate random appointment data and save it to a CSV file\n",
    "def generate_appointment_data_csv(filename, num_rows, buyer_ids, listing_ids):\n",
    "    data = {\n",
    "        'appointmentid': [],  # Appointment ID\n",
    "        'buyerid': [],        # Buyer ID\n",
    "        'listingid': [],      # Listing ID\n",
    "        'date': [],           # Date\n",
    "        'starttime': [],      # Start Time\n",
    "        'endtime': []         # End Time\n",
    "    }\n",
    "\n",
    "    # Use a set to ensure unique AppointmentIDs\n",
    "    existing_appointment_ids = set()\n",
    "\n",
    "    def generate_unique_appointment_id():\n",
    "        while True:\n",
    "            new_id = random.randint(100000000, 999999999)  # Generate a nine-digit ID\n",
    "            if new_id not in existing_appointment_ids:\n",
    "                existing_appointment_ids.add(new_id)\n",
    "                return new_id\n",
    "\n",
    "    for _ in range(num_rows):\n",
    "        # Randomly select a valid BuyerID and ListingID\n",
    "        buyer_id = random.choice(buyer_ids)\n",
    "        listing_id = random.choice(listing_ids)\n",
    "\n",
    "        # Randomly generate date and time\n",
    "        date = fake.date_between(start_date='-1y', end_date='today')\n",
    "        start_time = fake.time(pattern=\"%H:%M:%S\", end_datetime=None)\n",
    "        end_time = fake.time(pattern=\"%H:%M:%S\", end_datetime=None)\n",
    "\n",
    "        # Ensure end time is after start time\n",
    "        while end_time <= start_time:\n",
    "            end_time = fake.time(pattern=\"%H:%M:%S\", end_datetime=None)\n",
    "\n",
    "        # Populate data\n",
    "        data['appointmentid'].append(generate_unique_appointment_id())\n",
    "        data['buyerid'].append(buyer_id)\n",
    "        data['listingid'].append(listing_id)\n",
    "        data['date'].append(date)\n",
    "        data['starttime'].append(start_time)\n",
    "        data['endtime'].append(end_time)\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 600 rows of appointment data\n",
    "generate_appointment_data_csv('appointment_data.csv', 600, buyer_ids, listing_ids)\n",
    "\n",
    "print(\"Appointment CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "83038332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'transactionid': 1000, 'listingid': 1000, 'buyerid': 1000, 'transactiontype': 1000, 'transactiondatetime': 1000, 'price': 1000, 'commissions': 1000, 'status': 1000}\n",
      "Transaction CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#transaction\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# Read IDs from already generated listings_data.csv and buyers_data.csv\n",
    "listings_df = pd.read_csv('listings_data.csv')\n",
    "buyers_df = pd.read_csv('buyers_data.csv')\n",
    "\n",
    "listing_ids = listings_df['listingid'].tolist()\n",
    "buyer_ids = buyers_df['buyerid'].tolist()\n",
    "\n",
    "# Define a function to generate random transaction data and save it to a CSV file\n",
    "def generate_transaction_data_csv(filename, num_rows, listing_ids, buyer_ids):\n",
    "    data = {\n",
    "        'transactionid': [],      # Transaction ID\n",
    "        'listingid': [],          # Listing ID\n",
    "        'buyerid': [],            # Buyer ID\n",
    "        'transactiontype': [],    # Transaction Type\n",
    "        'transactiondatetime': [],# Transaction DateTime\n",
    "        'price': [],              # Price\n",
    "        'commissions': [],        # Commissions\n",
    "        'status': []              # Status\n",
    "    }\n",
    "\n",
    "    transaction_types = ['Sale', 'Lease', 'Rent']\n",
    "    statuses = ['Completed', 'Pending', 'Cancelled']\n",
    "\n",
    "    # Use a set to ensure unique TransactionIDs\n",
    "    existing_transaction_ids = set()\n",
    "\n",
    "    def generate_unique_transaction_id():\n",
    "        while True:\n",
    "            new_id = random.randint(100000, 999999)  # Generate a six-digit ID\n",
    "            if new_id not in existing_transaction_ids:\n",
    "                existing_transaction_ids.add(new_id)\n",
    "                return new_id\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly select a valid ListingID and BuyerID\n",
    "        listing_id = random.choice(listing_ids)\n",
    "        buyer_id = random.choice(buyer_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['transactionid'].append(generate_unique_transaction_id())\n",
    "        data['listingid'].append(listing_id)\n",
    "        data['buyerid'].append(buyer_id)\n",
    "        data['transactiontype'].append(random.choice(transaction_types))\n",
    "        transaction_datetime = fake.date_time_between(start_date='-1y', end_date='now')\n",
    "        data['transactiondatetime'].append(transaction_datetime)\n",
    "        price = round(random.uniform(100000, 5000000), 2)  # Generate a random price between 100,000 and 5,000,000\n",
    "        data['price'].append(price)\n",
    "        commissions = round(price * random.uniform(0.02, 0.06), 2)  # 2% to 6% commission\n",
    "        data['commissions'].append(commissions)\n",
    "        data['status'].append(random.choice(statuses))\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 1000 rows of transaction data\n",
    "generate_transaction_data_csv('transaction_data.csv', 1000, listing_ids, buyer_ids)\n",
    "\n",
    "print(\"Transaction CSV file generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98c5adc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'reviewid': 500, 'listingid': 500, 'rating': 500, 'comments': 500, 'reviewdate': 500}\n",
      "Reviews CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#review\n",
    "\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Read ListingID from the already generated listings_data.csv\n",
    "listings_df = pd.read_csv('listings_data.csv')\n",
    "listing_ids = listings_df['listingid'].tolist()\n",
    "\n",
    "# Define a function to generate random review data and save it to a CSV file\n",
    "def generate_reviews_data_csv(filename, num_rows, listing_ids):\n",
    "    data = {\n",
    "        'reviewid': [],      # Review ID\n",
    "        'listingid': [],     # Listing ID\n",
    "        'rating': [],        # Rating\n",
    "        'comments': [],      # Comments\n",
    "        'reviewdate': []     # Review Date\n",
    "    }\n",
    "\n",
    "    # Use a set to ensure unique ReviewIDs\n",
    "    existing_review_ids = set()\n",
    "\n",
    "    def generate_unique_review_id():\n",
    "        while True:\n",
    "            new_id = random.randint(100000, 999999)  # Generate a six-digit ID\n",
    "            if new_id not in existing_review_ids:\n",
    "                existing_review_ids.add(new_id)\n",
    "                return new_id\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly select a valid ListingID\n",
    "        listing_id = random.choice(listing_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['reviewid'].append(generate_unique_review_id())\n",
    "        data['listingid'].append(listing_id)\n",
    "        data['rating'].append(random.randint(1, 5))  # Generate a rating between 1 and 5\n",
    "        data['comments'].append(fake.paragraph(nb_sentences=3))  # Generate random comments\n",
    "        data['reviewdate'].append(fake.date_between(start_date='-1y', end_date='today'))  # Date from the past year\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 500 rows of review data\n",
    "generate_reviews_data_csv('reviews_data.csv', 500, listing_ids)\n",
    "\n",
    "print(\"Reviews CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1702b24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column lengths: {'revenueid': 500, 'transactionid': 500, 'revenuedate': 500, 'amount': 500, 'description': 500}\n",
      "Revenues CSV file generated.\n"
     ]
    }
   ],
   "source": [
    "#fake revenue\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker('en_US')\n",
    "\n",
    "# Read TransactionID from the already generated transaction_data.csv\n",
    "transactions_df = pd.read_csv('transaction_data.csv')\n",
    "transaction_ids = transactions_df['transactionid'].tolist()\n",
    "\n",
    "# Define a function to generate random revenue data and save it to a CSV file\n",
    "def generate_revenue_data_csv(filename, num_rows, transaction_ids):\n",
    "    data = {\n",
    "        'revenueid': [],        # Revenue ID\n",
    "        'transactionid': [],    # Transaction ID\n",
    "        'revenuedate': [],      # Revenue Date\n",
    "        'amount': [],           # Amount\n",
    "        'description': []       # Description\n",
    "    }\n",
    "\n",
    "    descriptions = [\n",
    "        'Real estate sale commission',\n",
    "        'Rental income',\n",
    "        'Lease signing bonus',\n",
    "        'Consultation fee',\n",
    "        'Property management fee',\n",
    "        'Investment return',\n",
    "        'Maintenance and service fee'\n",
    "    ]\n",
    "\n",
    "    # Use a set to ensure unique RevenueIDs\n",
    "    existing_revenue_ids = set()\n",
    "\n",
    "    def generate_unique_revenue_id():\n",
    "        while True:\n",
    "            new_id = random.randint(100000, 999999)  # Generate a six-digit ID\n",
    "            if new_id not in existing_revenue_ids:\n",
    "                existing_revenue_ids.add(new_id)\n",
    "                return new_id\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        # Randomly select a valid TransactionID\n",
    "        transaction_id = random.choice(transaction_ids)\n",
    "\n",
    "        # Populate data\n",
    "        data['revenueid'].append(generate_unique_revenue_id())\n",
    "        data['transactionid'].append(transaction_id)\n",
    "        data['revenuedate'].append(fake.date_between(start_date='-1y', end_date='today'))\n",
    "        data['amount'].append(round(random.uniform(5000, 200000), 2))  # Generate a random amount between 5,000 and 200,000\n",
    "        data['description'].append(random.choice(descriptions))\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Check if all columns have consistent lengths\n",
    "    column_lengths = {key: len(value) for key, value in data.items()}\n",
    "    print(\"Column lengths:\", column_lengths)\n",
    "\n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "# Generate a CSV file with 500 rows of revenue data\n",
    "generate_revenue_data_csv('revenues_data.csv', 500, transaction_ids)\n",
    "\n",
    "print(\"Revenues CSV file generated.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed4a2aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data insertion complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, MetaData, Table, insert\n",
    "\n",
    "# Create a connection to the PostgreSQL database\n",
    "engine = create_engine('postgresql://postgres:123@localhost:5432/mydatabase')\n",
    "metadata = MetaData(bind=engine)\n",
    "\n",
    "# Define tables\n",
    "offices = Table('offices', metadata, autoload_with=engine)\n",
    "expenses = Table('expenses', metadata, autoload_with=engine)\n",
    "agent = Table('agent', metadata, autoload_with=engine)\n",
    "users = Table('users', metadata, autoload_with=engine)\n",
    "services = Table('services', metadata, autoload_with=engine)\n",
    "sellers = Table('sellers', metadata, autoload_with=engine)\n",
    "buyers = Table('buyers', metadata, autoload_with=engine)\n",
    "properties = Table('properties', metadata, autoload_with=engine)\n",
    "property_images = Table('propertyimages', metadata, autoload_with=engine)\n",
    "listings = Table('listings', metadata, autoload_with=engine)\n",
    "appointments = Table('appointment', metadata, autoload_with=engine)\n",
    "transactions = Table('transactions', metadata, autoload_with=engine)\n",
    "reviews = Table('reviews', metadata, autoload_with=engine)\n",
    "openhouses = Table('openhouses', metadata, autoload_with=engine)\n",
    "revenues = Table('revenues', metadata, autoload_with=engine)\n",
    "\n",
    "# Load data from CSV files into tables\n",
    "def load_csv_to_table(csv_file, table):\n",
    "    data = pd.read_csv(csv_file)\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(insert(table), data.to_dict(orient='records'))\n",
    "\n",
    "# Insert data into tables\n",
    "load_csv_to_table('office_data.csv', offices)\n",
    "load_csv_to_table('expenses_data.csv', expenses)\n",
    "load_csv_to_table('agent_data.csv', agent)\n",
    "load_csv_to_table('users_data.csv', users)\n",
    "load_csv_to_table('services_data.csv', services)\n",
    "load_csv_to_table('sellers_data.csv', sellers)\n",
    "load_csv_to_table('buyers_data.csv', buyers)\n",
    "load_csv_to_table('properties_data.csv', properties)\n",
    "load_csv_to_table('property_images_data.csv', property_images)\n",
    "load_csv_to_table('listings_data.csv', listings)\n",
    "load_csv_to_table('appointment_data.csv', appointments)\n",
    "load_csv_to_table('transaction_data.csv', transactions)\n",
    "load_csv_to_table('reviews_data.csv', reviews)\n",
    "load_csv_to_table('openhouses_data.csv', openhouses)\n",
    "load_csv_to_table('revenues_data.csv', revenues)\n",
    "\n",
    "print(\"Data insertion complete.\")\n",
    "\n",
    "# Print table data as DataFrame\n",
    "def print_table_data(table):\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(table.select())\n",
    "        df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "        print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f2ce5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Office Table:\n",
      "     officeid                         name                       address  \\\n",
      "0        3435              Werner-Ferguson     47492 Bobby Mall Apt. 235   \n",
      "1        3010                Rodriguez LLC  316 Daniels Square Suite 247   \n",
      "2        4730   Johnson, Donaldson and May          816 Hutchinson Brook   \n",
      "3        6138  Vaughan, Daniels and Oneill             538 Sherry Divide   \n",
      "4        8297  Johnson, Moody and Buchanan   526 Jennifer Locks Apt. 762   \n",
      "..        ...                          ...                           ...   \n",
      "421      1076    Williams, Roth and Barnes                430 Rios Hills   \n",
      "422      3203                Johnson Group  5158 Stacey Prairie Apt. 955   \n",
      "423      3057               Jones-Matthews    887 Greg Streets Suite 171   \n",
      "424      6022                    Ramos PLC      2712 Carol Rue Suite 359   \n",
      "425      3219                  Nichols PLC              869 Potter Lakes   \n",
      "\n",
      "              city        state zipcode  numemployees expenses   profits  \n",
      "0          Quinton      Alabama   35130            54  6865.91  16608.98  \n",
      "1       Manchester         Iowa   52057            36  1215.12  16599.09  \n",
      "2    North Collins     New York   14111            70  7328.55  11329.98  \n",
      "3         Hartford  Connecticut    6167            26  4973.93  10894.39  \n",
      "4          Hulbert     Oklahoma   74441            96  4912.78  11441.26  \n",
      "..             ...          ...     ...           ...      ...       ...  \n",
      "421  Garrattsville     New York   13342            35  9146.85  14775.06  \n",
      "422      Masterson        Texas   79058            63  9438.07  14304.87  \n",
      "423         London     Kentucky   40745             7  7324.69  18432.27  \n",
      "424     Jonesville     Kentucky   41052            75  7950.22  14178.38  \n",
      "425       Pasadena     Maryland   21123            26  8326.32  13402.60  \n",
      "\n",
      "[426 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Office Table:\")\n",
    "print_table_data(office)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4be3624c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expenses Table:\n",
      "     expenseid  officeid expensedate  expensetype   amount  \\\n",
      "0            1      6891  2020-12-30    Utilities  7254.91   \n",
      "1            2      4097  2020-08-15     Salaries  9577.13   \n",
      "2            3      9490  2021-04-26     Supplies  6881.44   \n",
      "3            4      1001  2020-05-26         Rent  3846.55   \n",
      "4            5      4193  2023-03-16     Supplies  8894.79   \n",
      "..         ...       ...         ...          ...      ...   \n",
      "995        996      4811  2023-08-28  Maintenance  4948.48   \n",
      "996        997      6490  2022-04-27    Utilities  7609.78   \n",
      "997        998      2167  2021-06-14    Utilities  1157.30   \n",
      "998        999      8710  2020-04-30  Maintenance  6635.83   \n",
      "999       1000      4320  2023-04-24    Utilities  2723.80   \n",
      "\n",
      "                                           description  \n",
      "0    Look program pattern exist what day arrive din...  \n",
      "1    Must action general catch what politics discus...  \n",
      "2    Base able education mother movie top part. Sup...  \n",
      "3    Decade recognize lay choice among rise. Star t...  \n",
      "4    Program table stock shake read. Think school r...  \n",
      "..                                                 ...  \n",
      "995  Result soldier find one so class once. Work so...  \n",
      "996  Politics including house start serious. Togeth...  \n",
      "997  Perform decision rule peace beat create after....  \n",
      "998  Degree few reality medical plan score particul...  \n",
      "999  Believe establish office early organization. T...  \n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExpenses Table:\")\n",
    "print_table_data(expenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3db569c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Users Table:\n",
      "     userid  firstname  lastname         dob     phonenumber  \\\n",
      "0     18684      Holly     Simon  1955-02-10  (953) 051-9672   \n",
      "1     44179     Daniel    Bailey  1981-11-01  (396) 849-8539   \n",
      "2     55353    Jessica   Leonard  1964-08-11  (939) 470-5058   \n",
      "3     30527  Alexander   Kennedy  1972-03-09  (966) 872-2753   \n",
      "4     25576       Dana    Carter  1963-03-26  (139) 439-1551   \n",
      "..      ...        ...       ...         ...             ...   \n",
      "405   14741    Russell     Smith  1963-04-22  (408) 809-5235   \n",
      "406   38258     Robert     Curry  1981-01-16  (259) 378-0175   \n",
      "407   97009       Jose      Diaz  1999-03-02  (066) 070-7758   \n",
      "408   50179      David  Phillips  1958-04-14  (975) 804-1340   \n",
      "409   69263   Michelle   Johnson  1986-04-04  (324) 607-0962   \n",
      "\n",
      "                               email                        address  \\\n",
      "0          holly.simon31@example.com               50135 Klein Mall   \n",
      "1        daniel.bailey22@example.com     5453 Debra Union Suite 262   \n",
      "2      jessica.leonard17@example.com        23033 Dominguez Gateway   \n",
      "3    alexander.kennedy61@example.com               765 Barry Stream   \n",
      "4          dana.carter46@example.com       060 Burns Locks Apt. 391   \n",
      "..                               ...                            ...   \n",
      "405      russell.smith67@example.com   06694 Mejia Skyway Suite 458   \n",
      "406       robert.curry80@example.com              26680 Kevin Shore   \n",
      "407          jose.diaz26@example.com                528 Miller Pass   \n",
      "408     david.phillips55@example.com               1225 Mullins Key   \n",
      "409   michelle.johnson82@example.com  43537 Vargas Ridges Suite 629   \n",
      "\n",
      "              city statecode    zip      type  \n",
      "0         Stamford        CT   6927     Guest  \n",
      "1    Saint Anthony        ND  58566  Customer  \n",
      "2        Pinehurst        NC  28374     Guest  \n",
      "3          Hillman        MI  49746     Admin  \n",
      "4      Foster City        MI  49834     Admin  \n",
      "..             ...       ...    ...       ...  \n",
      "405      Lexington        KY  40580     Guest  \n",
      "406         Alcova        WY  82620  Customer  \n",
      "407        Arcadia        CA  91006  Employee  \n",
      "408     Cincinnati        OH  45245  Employee  \n",
      "409         Lander        WY  82520  Employee  \n",
      "\n",
      "[410 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUsers Table:\")\n",
    "print_table_data(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e1635c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Table:\n",
      "     agentid  officeid firstname  lastname     phonenumber  \\\n",
      "0      31690      2180       Amy   Walters  (925) 810-9096   \n",
      "1      84609      6869     James     Evans  (035) 276-2355   \n",
      "2      13386      9087   Deborah      Rice  (603) 515-0369   \n",
      "3      98792      8214  Jennifer    Miller  (468) 831-7536   \n",
      "4      48884      7883    Angela     Smith  (064) 931-6788   \n",
      "..       ...       ...       ...       ...             ...   \n",
      "995    58508      8869    Deanna   Hoffman  (963) 669-3775   \n",
      "996    81014      2922      Anna   Johnson  (915) 522-1316   \n",
      "997    31615      3368    Cheryl    Powers  (650) 119-4945   \n",
      "998    32137      6691       Amy   Roberts  (150) 751-5645   \n",
      "999    72679      6025    Gerald  Williams  (552) 081-5220   \n",
      "\n",
      "                           email  licensenumber yearsofexperience  \\\n",
      "0            amywalters@mail.com  TL58-09301117              5.54   \n",
      "1         james.evans@icloud.com  jh28-78884553             23.22   \n",
      "2                drice@gmail.com  tQ92-30796282             15.00   \n",
      "3              millerj@yahoo.com  Mc27-45719062             20.62   \n",
      "4        angela_smith@icloud.com  Kc21-44409483              5.20   \n",
      "..                           ...            ...               ...   \n",
      "995           dhoffman@yahoo.com  Qn59-28657494              6.65   \n",
      "996       anna.johnson@yahoo.com  HW90-19654358              9.97   \n",
      "997          cpowers@hotmail.com  UJ20-86740302              5.76   \n",
      "998           robertsa@gmail.com  RU21-36059445             12.29   \n",
      "999  gerald_williams@hotmail.com  cj46-47664675             22.82   \n",
      "\n",
      "    employmentstatus  \n",
      "0          Part-time  \n",
      "1           Contract  \n",
      "2           Contract  \n",
      "3          Full-time  \n",
      "4          Part-time  \n",
      "..               ...  \n",
      "995        Part-time  \n",
      "996        Full-time  \n",
      "997        Full-time  \n",
      "998         Contract  \n",
      "999        Part-time  \n",
      "\n",
      "[1000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAgent Table:\")\n",
    "print_table_data(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47fd3895",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sellers Table:\n",
      "     sellerid  userid askingprice sellingyears  numlistings\n",
      "0      223747   79869   551229.97         7.91           37\n",
      "1      941140   86093   832728.86         7.84           36\n",
      "2      332076   70351   306860.86         7.96           38\n",
      "3      193947   50161   321956.99         8.47           34\n",
      "4      643200   57745   623475.86         4.77           17\n",
      "..        ...     ...         ...          ...          ...\n",
      "495    182207   58834   327418.81         4.88           37\n",
      "496    339830   30942    80891.95        17.46           14\n",
      "497    847391   85151   144546.71        18.39           24\n",
      "498    865596   30527   497786.96         7.82           25\n",
      "499    541823   38679   757283.02         1.83           46\n",
      "\n",
      "[500 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSellers Table:\")\n",
    "print_table_data(sellers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec5e257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Buyers Table:\n",
      "      buyerid   userid     level purchasestatus      budget  \\\n",
      "0     6113857  77088.0      Gold    Considering   926415.65   \n",
      "1     6135635  56413.0      Gold    Considering   676829.14   \n",
      "2     6762001  96717.0  Platinum    Considering   930154.57   \n",
      "3     6062290  20728.0    Bronze    Considering   128244.44   \n",
      "4     4463457  21106.0    Bronze   Ready to Buy  2036488.44   \n",
      "...       ...      ...       ...            ...         ...   \n",
      "1337      838      NaN      None           None        None   \n",
      "1338      839      NaN      None           None        None   \n",
      "1339      840      NaN      None           None        None   \n",
      "1340      841      NaN      None           None        None   \n",
      "1341      842      NaN      None           None        None   \n",
      "\n",
      "     preferredpropertytype  moveindate  motivation petstatus  \n",
      "0                Apartment  2025-07-01  Investment     False  \n",
      "1                    Condo  2025-04-16  Relocation     False  \n",
      "2                Townhouse  2025-04-25  Relocation      True  \n",
      "3                Townhouse  2024-08-23    Upsizing      True  \n",
      "4                Apartment  2024-11-15  Downsizing     False  \n",
      "...                    ...         ...         ...       ...  \n",
      "1337                  None        None        None      None  \n",
      "1338                  None        None        None      None  \n",
      "1339                  None        None        None      None  \n",
      "1340                  None        None        None      None  \n",
      "1341                  None        None        None      None  \n",
      "\n",
      "[1342 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBuyers Table:\")\n",
    "print_table_data(buyers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf920ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Property Table:\n",
      "      buyerid   userid     level purchasestatus      budget  \\\n",
      "0     6113857  77088.0      Gold    Considering   926415.65   \n",
      "1     6135635  56413.0      Gold    Considering   676829.14   \n",
      "2     6762001  96717.0  Platinum    Considering   930154.57   \n",
      "3     6062290  20728.0    Bronze    Considering   128244.44   \n",
      "4     4463457  21106.0    Bronze   Ready to Buy  2036488.44   \n",
      "...       ...      ...       ...            ...         ...   \n",
      "1337      838      NaN      None           None        None   \n",
      "1338      839      NaN      None           None        None   \n",
      "1339      840      NaN      None           None        None   \n",
      "1340      841      NaN      None           None        None   \n",
      "1341      842      NaN      None           None        None   \n",
      "\n",
      "     preferredpropertytype  moveindate  motivation petstatus  \n",
      "0                Apartment  2025-07-01  Investment     False  \n",
      "1                    Condo  2025-04-16  Relocation     False  \n",
      "2                Townhouse  2025-04-25  Relocation      True  \n",
      "3                Townhouse  2024-08-23    Upsizing      True  \n",
      "4                Apartment  2024-11-15  Downsizing     False  \n",
      "...                    ...         ...         ...       ...  \n",
      "1337                  None        None        None      None  \n",
      "1338                  None        None        None      None  \n",
      "1339                  None        None        None      None  \n",
      "1340                  None        None        None      None  \n",
      "1341                  None        None        None      None  \n",
      "\n",
      "[1342 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProperty Table:\")\n",
    "print_table_data(property1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209099c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Listings Table:\n",
      "     listingid  agentid  sellerid  propertyid\n",
      "0    300652172    48884    444676    54261559\n",
      "1    870319240    28852    338952    96736577\n",
      "2    773785376    12803    438167    22927157\n",
      "3    351286999    40089    180880    81442990\n",
      "4    470042340    54435    731987    77440264\n",
      "..         ...      ...       ...         ...\n",
      "395  694135172    60867    311043    23087250\n",
      "396  251595536    48884    591351    43396133\n",
      "397  771695694    88884    873189    69912364\n",
      "398  736985386    70845    388573    36481186\n",
      "399  210505401    93514    483054    69912364\n",
      "\n",
      "[400 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nListings Table:\")\n",
    "print_table_data(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d076023",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Openhouses Table:\n",
      "     openhouseid  listingid        date starttime   endtime  \\\n",
      "0         336422  317049039  2023-11-01  05:36:38  12:56:50   \n",
      "1         785914  798168331  2024-06-03  15:43:47  20:26:12   \n",
      "2         766654  300543091  2023-11-02  01:02:01  03:10:50   \n",
      "3         693817  854451641  2024-01-23  06:03:58  18:15:21   \n",
      "4         111791  576501348  2024-03-11  20:46:21  23:53:42   \n",
      "..           ...        ...         ...       ...       ...   \n",
      "395       854323  936150376  2024-03-29  05:15:24  19:32:55   \n",
      "396       724058  771121464  2023-12-07  16:30:20  19:56:49   \n",
      "397       903256  708149937  2024-03-11  12:03:40  21:55:43   \n",
      "398       393738  858992671  2023-12-20  23:58:36  23:59:47   \n",
      "399       210509  936150376  2024-05-03  00:26:19  07:54:58   \n",
      "\n",
      "                                             attendees  \n",
      "0    Amy Wilson, Kayla Tucker, Kelly Yoder, Rebecca...  \n",
      "1    Louis Stewart, Renee Martinez, Nicole Allison,...  \n",
      "2    Christopher Martin, Carlos Coleman, Lisa Mendo...  \n",
      "3    Joshua Gilbert, Mr. Nathan Spencer, Bruce Morg...  \n",
      "4    Anthony Glass, Suzanne Harrington, Yolanda Gut...  \n",
      "..                                                 ...  \n",
      "395  Steven Rosales, Evan Russell, Taylor Walker, K...  \n",
      "396  Laura Barr, David Hill, Sean Jones, Amber Blan...  \n",
      "397  Colton Bell, Daniel Shelton, Samantha Rodrigue...  \n",
      "398  Lisa Pierce, Brenda Bird, Jose Lawson, Timothy...  \n",
      "399  Ashley Baldwin, Amber Bruce, Jessica Roman, St...  \n",
      "\n",
      "[400 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOpenhouses Table:\")\n",
    "print_table_data(openhouses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d51b2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transaction Table:\n",
      "     transactionid  listingid  buyerid   type        transactiondatetime  \\\n",
      "0           192192  234376708  1885468   Sale 2024-02-23 03:31:54.237514   \n",
      "1           487009  371712168  1190335   Rent 2024-04-17 00:13:43.674452   \n",
      "2           818129  583515796  2770824   Rent 2024-07-27 04:24:25.046961   \n",
      "3           299095  196891965  2338178   Rent 2024-04-08 01:01:01.255112   \n",
      "4           551066  980809755  6468741   Rent 2023-09-26 02:41:43.582686   \n",
      "..             ...        ...      ...    ...                        ...   \n",
      "395         884901  586015168  1190335   Rent 2024-06-21 08:18:02.302208   \n",
      "396         728491  960230782  2407987   Sale 2024-08-01 16:22:48.025480   \n",
      "397         595169  517924800  9371110   Sale 2023-11-16 10:26:27.201372   \n",
      "398         793475  940014413  3516697  Lease 2023-09-08 01:04:25.426087   \n",
      "399         847008  942495485  8188512  Lease 2024-04-27 19:38:33.182639   \n",
      "\n",
      "           price commissions     status  \n",
      "0    4742845.908   232184.39  Completed  \n",
      "1    4603102.583   123877.22  Completed  \n",
      "2    2334754.127   119020.65    Pending  \n",
      "3    4218897.707   210642.91  Completed  \n",
      "4    4899932.221   190308.48  Cancelled  \n",
      "..           ...         ...        ...  \n",
      "395  4295870.908   119566.56    Pending  \n",
      "396  2429502.242    84772.62    Pending  \n",
      "397  1329753.080    38775.97  Cancelled  \n",
      "398  3039491.658    73558.53  Completed  \n",
      "399  1212139.767    61612.22  Completed  \n",
      "\n",
      "[400 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTransaction Table:\")\n",
    "print_table_data(transaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8d78755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Revenues Table:\n",
      "     revenueid  transactionid revenuedate     amount  \\\n",
      "0       152001         920273  2024-01-20  189946.42   \n",
      "1       812273         761651  2024-05-07   32321.29   \n",
      "2       444287         190958  2023-12-27   81182.30   \n",
      "3       994402         217863  2024-01-26   87048.71   \n",
      "4       480324         663435  2024-01-29   62564.04   \n",
      "..         ...            ...         ...        ...   \n",
      "395     707319         681730  2023-10-09  187633.10   \n",
      "396     260553         611597  2024-02-15   44597.37   \n",
      "397     504778         432441  2024-01-07   72508.04   \n",
      "398     739512         509207  2023-09-09  119742.86   \n",
      "399     205645         511802  2024-01-05   48526.09   \n",
      "\n",
      "                     description  \n",
      "0              Investment return  \n",
      "1                  Rental income  \n",
      "2                  Rental income  \n",
      "3            Lease signing bonus  \n",
      "4        Property management fee  \n",
      "..                           ...  \n",
      "395  Real estate sale commission  \n",
      "396          Lease signing bonus  \n",
      "397             Consultation fee  \n",
      "398             Consultation fee  \n",
      "399          Lease signing bonus  \n",
      "\n",
      "[400 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRevenues Table:\")\n",
    "print_table_data(revenues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3a7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
